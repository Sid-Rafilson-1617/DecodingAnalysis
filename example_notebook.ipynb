{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88079d8",
   "metadata": {},
   "source": [
    "# Neural Decoding Analysis with LSTM Networks\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to perform neural decoding analysis using Long Short-Term Memory (LSTM) networks to predict behavioral variables from neural activity. The analysis includes:\n",
    "\n",
    "1. **Data Preprocessing**: Load and prepare spike train data and behavioral variables\n",
    "2. **Cross-Validation Setup**: Create training/testing splits with proper temporal structure\n",
    "3. **LSTM Model Training**: Train recurrent neural networks for decoding\n",
    "4. **Statistical Validation**: Compare model performance against null distributions\n",
    "5. **Results Visualization**: Generate plots and performance metrics\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Neural Decoding**: The process of inferring behavioral or cognitive states from neural activity patterns\n",
    "- **LSTM Networks**: Recurrent neural networks capable of learning long-term temporal dependencies\n",
    "- **Cross-Validation**: Statistical method to assess model generalizability\n",
    "- **Null Distribution**: Baseline comparison using temporally shuffled data to test statistical significance\n",
    "\n",
    "## Expected Outputs\n",
    "\n",
    "- Model performance metrics (RMSE) for each behavioral variable\n",
    "- Visualization of predicted vs. actual behavioral trajectories  \n",
    "- Statistical significance tests comparing true vs. shuffled data performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02894e",
   "metadata": {},
   "source": [
    "## Requirements and Setup\n",
    "\n",
    "### Dependencies\n",
    "- Python 3.7+\n",
    "- PyTorch\n",
    "- NumPy\n",
    "- Pandas\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "- SciPy\n",
    "\n",
    "### Data Structure Requirements\n",
    "Your data directory should contain preprocessed spike train data and behavioral measurements organized by mouse and session. The `core.py` module must contain:\n",
    "- `preprocess()` function for data loading\n",
    "- `LSTMDecoder` class\n",
    "- `SequenceDataset` class  \n",
    "- `train_LSTM()` function\n",
    "- `cv_split()` function\n",
    "- `plot_training()` function\n",
    "\n",
    "### Important Notes\n",
    "- **Update file paths** in the configuration section to match your local directory structure\n",
    "- Ensure GPU availability for faster training (optional but recommended)\n",
    "- This notebook generates substantial output files - ensure adequate disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63214356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.6.0+cu124\n",
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Standard scientific computing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch for neural network implementation\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom modules for neural decoding\n",
    "from core import *  # Contains preprocessing functions and LSTM model definitions\n",
    "\n",
    "# System and utility libraries\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Fix for OpenMP duplicate library issue on some systems\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Set plotting style for better visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure data directories\n",
    "# NOTE: Update these paths to match your local directory structure\n",
    "dir = r\"D:\\clickbait-mmz\"                    # Raw data directory\n",
    "save_dir = r\"C:\\Users\\smearlab\\analysis_code\\EncodingModels\\outputs\"  # Output directory\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b4413",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup and Initialization\n",
    "\n",
    "## Library Imports and Environment Configuration\n",
    "\n",
    "This section imports all necessary libraries and configures the computational environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17561e",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "## Experimental Parameters\n",
    "\n",
    "The following parameters define the experimental session and analysis settings. These should be adjusted based on your specific dataset and research objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENTAL PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Subject and session identification\n",
    "mouse = '6002'        # Subject ID\n",
    "session = '6'         # Session number\n",
    "\n",
    "# Time window parameters for binning neural data\n",
    "window_size = 2.0     # Time window size in seconds for spike binning\n",
    "step_size = 2         # Step size in seconds (2 = non-overlapping windows)\n",
    "\n",
    "# Sampling frequencies\n",
    "fs = 30000            # Sampling frequency for spike data (Hz)\n",
    "sfs = 1000            # Sampling frequency for behavioral/sniff data (Hz)\n",
    "\n",
    "# Unit selection criteria\n",
    "use_units = 'good'    # Options: 'all', 'good', or specific unit number\n",
    "\n",
    "# Cross-validation parameters\n",
    "k_CV = 3              # Number of cross-validation folds\n",
    "n_blocks = 3          # Number of temporal blocks for cross-validation\n",
    "\n",
    "# Null distribution parameters\n",
    "shift = 2             # Temporal shift for null distribution (0 = no shift, >0 = shuffled)\n",
    "\n",
    "# Model configuration\n",
    "model_input = 'neural'  # Input type: 'neural' (spikes->behavior) or 'behavioral' (behavior->spikes)\n",
    "\n",
    "# Behavioral variables to decode\n",
    "use_behaviors = [\n",
    "    'position_x',     # X-coordinate position\n",
    "    'position_y',     # Y-coordinate position  \n",
    "    'velocity_x',     # X-component of velocity\n",
    "    'velocity_y',     # Y-component of velocity\n",
    "    'sns'             # Sniff rate\n",
    "]\n",
    "\n",
    "# Create output directory structure\n",
    "save_dir = os.path.join(save_dir, mouse, session, f'shift_{shift}')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration complete for Mouse {mouse}, Session {session}\")\n",
    "print(f\"Output directory: {save_dir}\")\n",
    "print(f\"Behavioral variables: {use_behaviors}\")\n",
    "print(f\"Cross-validation: {k_CV} folds, {n_blocks} blocks per fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af4c77",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Loading and Preparing Neural Data\n",
    "\n",
    "This section preprocesses the raw data to extract:\n",
    "- Spike counts binned at specified temporal resolution\n",
    "- Behavioral variables and their names\n",
    "- Trial identifications for temporal organization\n",
    "- Neuron names and metadata from manual curation\n",
    "\n",
    "The `preprocess()` function handles the complex data loading and alignment procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a370fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "# Preprocessing the data to get spike counts, behavioral variables and names, \n",
    "# trial IDs, neuron names, and neuron info from manual curation\n",
    "# \n",
    "# Returns:\n",
    "# - counts: Neural spike counts binned in time windows [time_bins x neurons]\n",
    "# - variables: List of behavioral variables aligned to neural data\n",
    "# - variable_names: Names of behavioral variables\n",
    "# - trial_ids: Trial identification for temporal organization\n",
    "# - neu_names: Names of all recorded neurons\n",
    "# - neu_info: Metadata for each neuron (including brain region)\n",
    "\n",
    "counts, variables, variable_names, trial_ids, neu_names, neu_info = preprocess(\n",
    "    dir, save_dir, mouse, session, window_size, step_size, use_units\n",
    ")\n",
    "\n",
    "print(f\"Data preprocessing complete:\")\n",
    "print(f\"  - Neural data shape: {counts.shape} (time_bins x neurons)\")\n",
    "print(f\"  - Behavioral variables: {len(variable_names)}\")\n",
    "print(f\"  - Total neurons: {len(neu_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4fea0",
   "metadata": {},
   "source": [
    "## Brain Region Selection\n",
    "\n",
    "**Choose which brain region to analyze from 'OB' (Olfactory Bulb) or 'HC' (Hippocampus)**\n",
    "\n",
    "This code filters neurons based on their recorded brain region and applies temporal shuffling for null distribution construction when `shift > 0`. The circular shift of spike rates breaks the temporal relationship between neural activity and behavior, creating a null distribution for statistical hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling spike rates by 761 time bins\n",
      "spike rates shape (# time bins, # neurons): (917, 29)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BRAIN REGION SELECTION AND TEMPORAL SHUFFLING\n",
    "# =============================================================================\n",
    "\n",
    "# Filter neurons based on brain region (HC = Hippocampus, OB = Olfactory Bulb)\n",
    "use_units = [key for key, value in neu_info.items() if value['area'] == 'HC']\n",
    "spike_rates = counts[:, np.isin(neu_names, use_units)]\n",
    "\n",
    "# Apply temporal shift for null distribution if shift > 0\n",
    "# This breaks the temporal relationship between neural activity and behavior\n",
    "if shift > 0:\n",
    "    random_roll = np.random.randint(0, spike_rates.shape[0])\n",
    "    print(f'Rolling spike rates by {random_roll} time bins for null distribution')\n",
    "    spike_rates = np.roll(spike_rates, random_roll, axis=0)\n",
    "    \n",
    "print(f'Final spike rates shape (time bins × neurons): {spike_rates.shape}')\n",
    "print(f'Using {len(use_units)} neurons from hippocampus (HC)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e961be2",
   "metadata": {},
   "source": [
    "## Extracting Behavioral Variables\n",
    "\n",
    "This section extracts and organizes the behavioral variables specified in the configuration. The variables are stacked into a single matrix for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9eaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables: ['position_x', 'position_y', 'velocity_x', 'velocity_y', 'sns', 'latency', 'phase', 'speed', 'click']\n",
      "behavior shape (# time bins, # dims): (917, 5)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BEHAVIORAL VARIABLE EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "print(f'Available behavioral variables: {variable_names}')\n",
    "\n",
    "# Extract each behavioral component as specified in use_behaviors\n",
    "behavior_components = []\n",
    "\n",
    "if 'position_x' in use_behaviors:\n",
    "    pos_x = np.array(variables[variable_names.index('position_x')])\n",
    "    behavior_components.append(pos_x)\n",
    "    print(f\"Added position_x with shape: {pos_x.shape}\")\n",
    "\n",
    "if 'position_y' in use_behaviors:\n",
    "    pos_y = np.array(variables[variable_names.index('position_y')])\n",
    "    behavior_components.append(pos_y)\n",
    "    print(f\"Added position_y with shape: {pos_y.shape}\")\n",
    "\n",
    "if 'velocity_x' in use_behaviors:\n",
    "    vel_x = np.array(variables[variable_names.index('velocity_x')])\n",
    "    behavior_components.append(vel_x)\n",
    "    print(f\"Added velocity_x with shape: {vel_x.shape}\")\n",
    "\n",
    "if 'velocity_y' in use_behaviors:\n",
    "    vel_y = np.array(variables[variable_names.index('velocity_y')])\n",
    "    behavior_components.append(vel_y)\n",
    "    print(f\"Added velocity_y with shape: {vel_y.shape}\")\n",
    "\n",
    "if 'sns' in use_behaviors:\n",
    "    sniff_rate = np.array(variables[variable_names.index('sns')])\n",
    "    behavior_components.append(sniff_rate)\n",
    "    print(f\"Added sniff rate with shape: {sniff_rate.shape}\")\n",
    "\n",
    "# Stack all behavioral components into a single matrix\n",
    "behavior = np.stack(behavior_components, axis=1)\n",
    "print(f'Final behavior matrix shape (time bins × behavioral dimensions): {behavior.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0984f3d",
   "metadata": {},
   "source": [
    "## Model Input/Output Configuration\n",
    "\n",
    "**Building list of arguments to pass to the model**\n",
    "\n",
    "This is where the input $\\mathbf{X}$ and output $y$ variables for the model are defined.\n",
    "\n",
    "$$y = f(\\mathbf{X})$$\n",
    "\n",
    "where $f(\\cdot)$ is a neural network learned from the data to minimize some cost function.\n",
    "\n",
    "**For Neural Decoding:**\n",
    "- Input $\\mathbf{X}$: Neural spike rates (time bins × neurons)\n",
    "- Output $y$: Behavioral variables (time bins × behavioral dimensions)\n",
    "\n",
    "The model learns to predict behavioral states from neural activity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053301ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CROSS-VALIDATION SETUP AND MODEL INPUT/OUTPUT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize list to store arguments for each cross-validation fold\n",
    "arg_list = []\n",
    "\n",
    "# Loop through each cross-validation fold to build datasets\n",
    "for k in range(k_CV):\n",
    "    # Split spike rates data using temporal cross-validation\n",
    "    rates_train, rates_test, train_switch_ind, test_switch_ind = cv_split(\n",
    "        spike_rates, k, k_CV, n_blocks\n",
    "    )\n",
    "    \n",
    "    # Split behavioral data using the same temporal structure\n",
    "    behavior_train, behavior_test, _, _ = cv_split(\n",
    "        behavior, k, k_CV, n_blocks\n",
    "    )\n",
    "\n",
    "    # Create save directory for model outputs\n",
    "    current_save_path = os.path.join(save_dir, \"model fits\")\n",
    "    os.makedirs(current_save_path, exist_ok=True)\n",
    "\n",
    "    # Configure input (X) and output (y) based on model type\n",
    "    if model_input == 'neural':\n",
    "        # Neural decoding: Neural activity -> Behavior\n",
    "        y_train = behavior_train\n",
    "        y_test = behavior_test\n",
    "        X_train = rates_train\n",
    "        X_test = rates_test\n",
    "    elif model_input == 'behavioral':\n",
    "        # Behavioral encoding: Behavior -> Neural activity\n",
    "        y_train = rates_train\n",
    "        y_test = rates_test\n",
    "        X_train = behavior_train\n",
    "        X_test = behavior_test\n",
    "\n",
    "    # Store all arguments needed for this fold\n",
    "    arg_list.append((\n",
    "        X_train, X_test, y_train, y_test, \n",
    "        train_switch_ind, test_switch_ind, \n",
    "        current_save_path, None, shift\n",
    "    ))\n",
    "\n",
    "print(f\"Cross-validation setup complete: {k_CV} folds prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9193bc8c",
   "metadata": {},
   "source": [
    "# LSTM Model Functions\n",
    "\n",
    "## Visualization and Training Functions\n",
    "\n",
    "This section defines helper functions for:\n",
    "1. **Visualization**: Plotting model predictions against true values\n",
    "2. **Training**: Complete pipeline for training and evaluating LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(targets, predictions, test_switch_ind, sequence_length, save_path, k, shift):\n",
    "    \"\"\"\n",
    "    Plot model predictions against true values for visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    targets : numpy.ndarray\n",
    "        True behavioral values (time_steps x behavioral_dimensions)\n",
    "    predictions : numpy.ndarray  \n",
    "        Model predictions (time_steps x behavioral_dimensions)\n",
    "    test_switch_ind : list\n",
    "        Indices where test data switches between blocks/trials\n",
    "    sequence_length : int\n",
    "        Length of sequences used in LSTM training\n",
    "    save_path : str\n",
    "        Directory path to save the plot\n",
    "    k : int\n",
    "        Cross-validation fold number\n",
    "    shift : int\n",
    "        Temporal shift parameter for null distribution\n",
    "    \"\"\"\n",
    "    # Adjust test switch indices to account for sequence length\n",
    "    adjusted_test_switch_ind = [ind - sequence_length * k for k, ind in enumerate(test_switch_ind)]\n",
    "    \n",
    "    # Get number of behavioral dimensions\n",
    "    behavior_dim = targets.shape[1]\n",
    "    \n",
    "    # Create subplots for each behavioral dimension\n",
    "    _, ax = plt.subplots(behavior_dim, 1, figsize=(20, 10), sharex=True)\n",
    "    if behavior_dim == 1:\n",
    "        ax = [ax]\n",
    "    \n",
    "    # Plot true vs predicted for each dimension\n",
    "    for i in range(behavior_dim):\n",
    "        ax[i].plot(targets[:, i], label='True', color='crimson')\n",
    "        ax[i].plot(predictions[:, i], label='Predicted')\n",
    "        \n",
    "        # Add vertical lines at test block boundaries\n",
    "        for ind in adjusted_test_switch_ind:\n",
    "            ax[i].axvline(ind, color='grey', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Remove y-axis ticks if too many dimensions\n",
    "    if behavior_dim > 4:\n",
    "        for a in ax:\n",
    "            a.set_yticks([])\n",
    "    \n",
    "    plt.xlabel('Time')\n",
    "    ax[0].legend(loc='upper right')\n",
    "    sns.despine()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(save_path, f'lstm_predictions_k_{k}_shift_{shift}.png'), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa20d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fold(X_train, X_test, y_train, y_test, train_switch_ind, test_switch_ind, current_save_path,\n",
    "        device=None, shift=0, hidden_dim=8, num_layers=2, dropout=0.1, sequence_length=3, target_index=-1, \n",
    "        batch_size=64, lr=0.01, num_epochs=300, patience=10, min_delta=0.01, factor=0.5, plot_predictions=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate LSTM model for a single cross-validation fold.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : numpy.ndarray\n",
    "        Training and testing input data (neural activity or behavior)\n",
    "    y_train, y_test : numpy.ndarray\n",
    "        Training and testing target data (behavior or neural activity)\n",
    "    train_switch_ind, test_switch_ind : list\n",
    "        Indices marking boundaries between temporal blocks\n",
    "    current_save_path : str\n",
    "        Directory to save model outputs and plots\n",
    "    device : str, optional\n",
    "        GPU device to use ('0', '1', etc.), None for auto-detection\n",
    "    shift : int\n",
    "        Temporal shift for null distribution (0 = no shift)\n",
    "    hidden_dim : int\n",
    "        Number of hidden units in LSTM layers\n",
    "    num_layers : int\n",
    "        Number of LSTM layers\n",
    "    dropout : float\n",
    "        Dropout probability for regularization\n",
    "    sequence_length : int\n",
    "        Length of input sequences for LSTM\n",
    "    target_index : int\n",
    "        Which time step to predict (-1 = last time step)\n",
    "    batch_size : int\n",
    "        Training batch size\n",
    "    lr : float\n",
    "        Learning rate for optimizer\n",
    "    num_epochs : int\n",
    "        Maximum number of training epochs\n",
    "    patience : int\n",
    "        Early stopping patience (epochs without improvement)\n",
    "    min_delta : float\n",
    "        Minimum improvement threshold for early stopping\n",
    "    factor : float\n",
    "        Learning rate reduction factor\n",
    "    plot_predictions : bool\n",
    "        Whether to generate prediction plots\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rmse : numpy.ndarray\n",
    "        Root mean squared error for each output dimension\n",
    "    targets : numpy.ndarray\n",
    "        True target values from test set\n",
    "    predictions : numpy.ndarray\n",
    "        Model predictions on test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # =============================================================================\n",
    "    # DEVICE SETUP AND MODEL INITIALIZATION\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Set the device for computation (GPU if available, otherwise CPU)\n",
    "    if device:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = device\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create the LSTM model with specified architecture\n",
    "    lstm_model = LSTMDecoder(\n",
    "        input_dim=X_train.shape[1], \n",
    "        hidden_dim=hidden_dim, \n",
    "        output_dim=y_train.shape[1], \n",
    "        num_layers=num_layers, \n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # =============================================================================\n",
    "    # TRAINING DATA PREPARATION\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Prepare the training data for LSTM with proper temporal structure\n",
    "    blocks = [(train_switch_ind[i], train_switch_ind[i + 1]) for i in range(len(train_switch_ind) - 1)]\n",
    "    train_dataset = SequenceDataset(X_train, y_train, blocks, sequence_length, target_index)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=min(batch_size, len(train_dataset)), \n",
    "        shuffle=False, \n",
    "        num_workers=0, \n",
    "        pin_memory=True, \n",
    "        prefetch_factor=None\n",
    "    )\n",
    "\n",
    "    # =============================================================================\n",
    "    # MODEL TRAINING\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Train the LSTM model\n",
    "    start_train = time.time()\n",
    "    trained_lstm_model, lstm_history = train_LSTM(\n",
    "        lstm_model, train_loader, device, \n",
    "        lr=lr, epochs=num_epochs, patience=patience, \n",
    "        min_delta=min_delta, factor=factor, verbose=False\n",
    "    )\n",
    "    print(f\"\\nTraining time: {time.time() - start_train:.2f}s fold={k} shift={shift}\", flush=True)\n",
    "    \n",
    "    # Free up memory\n",
    "    del lstm_model, train_dataset, train_loader\n",
    "\n",
    "    # Plot training history if requested\n",
    "    if plot_predictions:\n",
    "        plot_training(lstm_history, current_save_path, shift, k)\n",
    "\n",
    "    # =============================================================================\n",
    "    # TEST DATA PREPARATION AND EVALUATION\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Prepare the test data for LSTM evaluation\n",
    "    test_blocks = [(test_switch_ind[i], test_switch_ind[i + 1]) for i in range(len(test_switch_ind) - 1)]\n",
    "    test_dataset = SequenceDataset(X_test, y_test, test_blocks, sequence_length, target_index)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=min(batch_size, len(test_dataset)), \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Generate predictions on the test set\n",
    "    trained_lstm_model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            preds = trained_lstm_model(X_batch)\n",
    "            predictions.append(preds.cpu().numpy())\n",
    "            targets.append(y_batch.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "\n",
    "    # Clean up memory\n",
    "    del trained_lstm_model, test_dataset, test_loader\n",
    "\n",
    "    # Generate visualization plots\n",
    "    plot_preds(targets, predictions, test_switch_ind, sequence_length, current_save_path, k, shift)\n",
    "\n",
    "    # Calculate RMSE for each output dimension\n",
    "    diffs = predictions - targets\n",
    "    rmse = np.sqrt(np.mean(diffs ** 2, axis=0))\n",
    "\n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Total time {time.time() - start_train:.2f}s for fold={k} shift={shift}\\n\\n\\n\", flush=True)\n",
    "\n",
    "    return rmse, targets, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941f4d4",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "## Running the model across all folds and saving results\n",
    "\n",
    "This section executes the LSTM training pipeline across all cross-validation folds and saves the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c6e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1/3...\n",
      "Using device: cuda\n",
      "\n",
      "Training time: 2.46s fold=0 shift=2\n",
      "Total time 3.69s for fold=0 shift=2\n",
      "\n",
      "\n",
      "\n",
      "Processing fold 2/3...\n",
      "Using device: cuda\n",
      "\n",
      "Training time: 3.38s fold=1 shift=2\n",
      "Total time 5.19s for fold=1 shift=2\n",
      "\n",
      "\n",
      "\n",
      "Processing fold 3/3...\n",
      "Using device: cuda\n",
      "\n",
      "Training time: 2.99s fold=2 shift=2\n",
      "Total time 4.11s for fold=2 shift=2\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL TRAINING AND RESULT SAVING\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "results_save_dir = os.path.join(save_dir, \"results\")\n",
    "os.makedirs(results_save_dir, exist_ok=True)\n",
    "\n",
    "# Process each cross-validation fold\n",
    "for k in range(k_CV):\n",
    "    print(f\"Processing fold {k + 1}/{k_CV}...\")\n",
    "    \n",
    "    # Train LSTM model and get results\n",
    "    rmse, targets, predictions = process_fold(*arg_list[k])\n",
    "\n",
    "    # Define file paths for saving results\n",
    "    results_file = f'results_shift{shift}_fold{k}.npz'\n",
    "    results_path = os.path.join(results_save_dir, results_file)\n",
    "    \n",
    "    # Save results as compressed .npz file\n",
    "    # Contains: RMSE values, true targets, and model predictions\n",
    "    np.savez_compressed(\n",
    "        results_path, \n",
    "        rmse=rmse, \n",
    "        targets=targets, \n",
    "        predictions=predictions\n",
    "    )\n",
    "\n",
    "    # Store metadata in results table\n",
    "    results.append({\n",
    "        'mouse': mouse,\n",
    "        'session': session,\n",
    "        'shift': shift,\n",
    "        'fold': k,\n",
    "        'results_file': results_file,\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and save as CSV for easy loading\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(os.path.join(results_save_dir, 'results.csv'), index=False)\n",
    "\n",
    "print(f\"\\nAll {k_CV} folds complete!\")\n",
    "print(f\"Results saved to: {results_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dd4b2",
   "metadata": {},
   "source": [
    "# Statistical Analysis and Model Comparison\n",
    "\n",
    "## Comparing True vs. Null Model Performance\n",
    "\n",
    "This section performs statistical tests to determine if the model's performance on real data is significantly better than chance. We compare RMSE values from the true data (shift=0) against the null distribution (shift>0).\n",
    "\n",
    "**Note:** RMSE is a list with `n_folds` elements, where each element contains RMSE values for each behavioral dimension. The index corresponds to the target dimension being decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f46a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shifts: 2\n",
      "dim 0: true - null RMSE = -14.6328, p-value = 0.0201, statistic = -2.3238\n",
      "dim 1: true - null RMSE = -6.1701, p-value = 0.1967, statistic = -1.2910\n",
      "dim 2: true - null RMSE = -0.0024, p-value = 0.7963, statistic = -0.2582\n",
      "dim 3: true - null RMSE = 0.0127, p-value = 0.4386, statistic = 0.7746\n",
      "dim 4: true - null RMSE = 0.5347, p-value = 0.1213, statistic = 1.5492\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STATISTICAL ANALYSIS: TRUE vs. NULL PERFORMANCE\n",
    "# =============================================================================\n",
    "\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "# Parameters for statistical analysis\n",
    "n_dims = 5  # Number of behavioral dimensions to analyze\n",
    "\n",
    "# Path to session results directory (adjust as needed)\n",
    "session_dir = r\"C:\\Users\\smearlab\\analysis_code\\EncodingModels\\outputs\\6002\\6\"\n",
    "\n",
    "# Get list of all shift conditions (shift_0, shift_1, etc.)\n",
    "shifts = os.listdir(session_dir)\n",
    "n_shifts = len(shifts)\n",
    "\n",
    "print(f'Number of shifts: {n_shifts - 1}')  # -1 because shift_0 is true data\n",
    "\n",
    "# Compare performance for each behavioral dimension\n",
    "for dim in range(n_dims):\n",
    "    true_rmse = []   # RMSE values for true data (shift=0)\n",
    "    null_rmse = []   # RMSE values for null distribution (shift>0)\n",
    "    \n",
    "    # Loop through all shift conditions\n",
    "    for shift in range(n_shifts):\n",
    "        shift_dir = os.path.join(session_dir, f'shift_{shift}', 'results')\n",
    "        \n",
    "        # Count number of cross-validation folds\n",
    "        n_folds = len([f for f in os.listdir(shift_dir) \n",
    "                      if f.startswith('results') and f.endswith('.npz')])\n",
    "        \n",
    "        # Load RMSE for each fold\n",
    "        for fold in range(n_folds):\n",
    "            results_file = os.path.join(shift_dir, f'results_shift{shift}_fold{fold}.npz')\n",
    "            rmse = np.load(results_file)['rmse'][dim]\n",
    "\n",
    "            # Separate true vs null distributions\n",
    "            if shift == 0:\n",
    "                true_rmse.append(rmse)  # True data\n",
    "            else:\n",
    "                null_rmse.append(rmse)  # Null distribution\n",
    "\n",
    "    # Perform statistical test using Wilcoxon rank-sum test\n",
    "    # This is a non-parametric test for comparing two independent samples\n",
    "    stat, p = ranksums(true_rmse, null_rmse)\n",
    "    \n",
    "    # Calculate effect size (difference in means)\n",
    "    mean_true = np.mean(true_rmse)\n",
    "    mean_null = np.mean(null_rmse)\n",
    "    effect_size = mean_true - mean_null\n",
    "    \n",
    "    print(f'Dimension {dim}: true - null RMSE = {effect_size:.4f}, '\n",
    "          f'p-value = {p:.4f}, statistic = {stat:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb196a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Conclusions\n",
    "\n",
    "## What This Analysis Tells Us\n",
    "\n",
    "This notebook implements a complete pipeline for neural decoding using LSTM networks. The key insights from this analysis:\n",
    "\n",
    "### Model Performance\n",
    "- **RMSE values** quantify how well the model predicts behavioral variables from neural activity\n",
    "- **Lower RMSE** indicates better prediction accuracy\n",
    "- **Cross-validation** ensures the model generalizes beyond the training data\n",
    "\n",
    "### Statistical Significance\n",
    "- **Null distribution** (temporally shuffled data) provides a baseline for comparison\n",
    "- **Wilcoxon rank-sum test** determines if true performance is significantly better than chance\n",
    "- **Effect size** measures the practical significance of the improvement\n",
    "\n",
    "### Behavioral Variables Decoded\n",
    "1. **Position (X, Y)**: Spatial location of the animal\n",
    "2. **Velocity (X, Y)**: Movement speed and direction\n",
    "3. **Sniff Rate**: Olfactory sampling behavior\n",
    "\n",
    "## Interpretation Guidelines\n",
    "\n",
    "### Strong Decoding Performance\n",
    "- **Low RMSE** and **significant p-values** indicate the neural population contains information about the behavior\n",
    "- **Large effect sizes** suggest the relationship is not only significant but also meaningful\n",
    "\n",
    "### Poor Decoding Performance\n",
    "- **High RMSE** or **non-significant p-values** may indicate:\n",
    "  - Neural population doesn't encode this behavior\n",
    "  - Temporal resolution is inappropriate\n",
    "  - Model architecture needs adjustment\n",
    "  - Insufficient data for reliable estimation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Parameter tuning**: Adjust LSTM architecture, learning rate, or sequence length\n",
    "2. **Feature engineering**: Consider different spike binning windows or normalization\n",
    "3. **Model comparison**: Test against linear decoders or other architectures\n",
    "4. **Population analysis**: Examine individual neuron contributions\n",
    "5. **Temporal dynamics**: Analyze how decoding accuracy changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6ace7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
